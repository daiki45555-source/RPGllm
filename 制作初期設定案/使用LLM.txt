📝 【技術仕様書：基幹AIエンジン選定】
1. 採用モデル
名称: Qwen 2.5 - 14B (Instruct)

分類: オープンソースLLM (ローカル実行型)

役割: ゲームマスター（GM）、ナラティブ生成、カルマ値計算、S.I.（自由入力）の解釈

コスト: ソフトウェア使用料 0円（完全無料）

2. 選定理由（Why This?）
本プロジェクトの要件である「ダークファンタジーの耽美な表現」と「ゲームとしての応答速度」を両立する唯一の解として選定。

日本語性能: 14B（中規模）ながら、70Bクラスに匹敵する日本語理解力を持つ。マリアンヌの敬語や、ジャックの粗暴な口調の書き分けが可能。

検閲耐性: 「これはゲームの演出である」というSystem Prompt（指示）に従順であり、暴力・流血・絶望的な描写を拒否せずに生成できる（ここが最重要）。

処理速度: RTX 4070環境下において、プレイヤーを待たせない「人間が読むより速い生成速度」を保証。

3. ターゲット環境（Hardware）
想定プレイヤー層：VRChatユーザー（ハイエンドPC所持者）

推奨GPU: NVIDIA GeForce RTX 3060 (12GB) 以上

開発機（貴殿）: RTX 4070 (12GB) -> 判定：【Sランク（快適）】

実行方式: 「ローカル分散処理（各々のPCで動かす）」

友人たちのPCスペックが高いため、各自のPC内でLLMを完結させる方式を採用。

サーバー遅延ゼロ、通信コストゼロの理想的な環境。

4. 実装・運用ツール
実行エンジン: KoboldCPP

理由: インストール不要（exeのみ）、軽量、API機能標準搭載。

連携: 将来的にUnityやUnreal Engine、あるいはPythonで作るゲーム本体から、このKoboldCPPへ命令を送る形で実装する。

5. プロデューサー（あなた）へのメリット
権利関係: ローカルモデルのため、生成された文章の権利は100%開発者に帰属する。

ブラックボックス化: ゲームロジック（カルマ計算式など）を外部APIに送らないため、ネタバレや流出のリスクがない。

拡張性: 将来的に「もっと頭のいいモデル」が出ても、ファイルを差し替えるだけでアップグレード可能。